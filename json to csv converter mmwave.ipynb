{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1gZuor13U_6-lyNst3VIuCdnQwBcvW2H3","authorship_tag":"ABX9TyN5SxVgUsxuEX1dCJCCe5Yp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4ohghr_1WF75"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import json\n","import numpy as np\n","import pandas as pd\n","import os\n","\n","def extract_frame_features(frame):\n","    \"\"\"Extract per-frame spatial features from mmWave frame data.\"\"\"\n","    points = np.array(frame[\"frameData\"].get(\"pointCloud\", []))\n","\n","    if points.size == 0:\n","        return None\n","\n","    x, y, z, doppler, snr = points[:, 0], points[:, 1], points[:, 2], points[:, 3], points[:, 4]\n","\n","    return {\n","        \"frameNum\": frame[\"frameData\"].get(\"frameNum\"),\n","        \"timestamp\": frame.get(\"timestamp\"),\n","        \"num_points\": len(points),\n","        \"mean_x\": np.mean(x),  \"mean_y\": np.mean(y),  \"mean_z\": np.mean(z),\n","        \"var_x\": np.var(x),    \"var_y\": np.var(y),    \"var_z\": np.var(z),\n","        \"mean_doppler\": np.mean(doppler),\n","        \"var_doppler\": np.var(doppler),\n","        \"mean_snr\": np.mean(snr),\n","        \"var_snr\": np.var(snr),\n","        \"bbox_x_min\": np.min(x), \"bbox_x_max\": np.max(x),\n","        \"bbox_y_min\": np.min(y), \"bbox_y_max\": np.max(y),\n","        \"bbox_z_min\": np.min(z), \"bbox_z_max\": np.max(z),\n","    }\n","\n","\n","def compute_velocity_acceleration(df):\n","    df = df.sort_values(\"frameNum\").reset_index(drop=True)\n","\n","    dt = np.diff(df[\"timestamp\"], prepend=df[\"timestamp\"].iloc[0]) / 1000.0\n","    dt[dt == 0] = 1e-6  # avoid divide by zero\n","\n","    for axis in [\"x\", \"y\", \"z\"]:\n","        df[f\"vel_{axis}\"] = df[f\"mean_{axis}\"].diff().fillna(0) / dt\n","        df[f\"acc_{axis}\"] = df[f\"vel_{axis}\"].diff().fillna(0) / dt\n","\n","    df[\"vel_mag\"] = np.sqrt(df[\"vel_x\"] ** 2 + df[\"vel_y\"] ** 2 + df[\"vel_z\"] ** 2)\n","    df[\"acc_mag\"] = np.sqrt(df[\"acc_x\"] ** 2 + df[\"acc_y\"] ** 2 + df[\"acc_z\"] ** 2)\n","\n","    return df\n","\n","\n","def aggregate_features(df, window_seconds=1.0):\n","    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n","\n","    start_time = df[\"timestamp\"].iloc[0]\n","    df[\"time_window\"] = ((df[\"timestamp\"] - start_time) / 1000.0 // window_seconds).astype(int)\n","\n","    agg_funcs = {\n","        \"num_points\": [\"mean\", \"std\"],\n","        \"mean_x\": [\"mean\", \"std\"], \"mean_y\": [\"mean\", \"std\"], \"mean_z\": [\"mean\", \"std\"],\n","        \"vel_mag\": [\"mean\", \"max\", \"std\"],\n","        \"acc_mag\": [\"mean\", \"max\", \"std\"],\n","        \"mean_doppler\": [\"mean\", \"std\"],\n","        \"mean_snr\": [\"mean\", \"std\"],\n","    }\n","\n","    agg_df = df.groupby(\"time_window\").agg(agg_funcs)\n","    agg_df.columns = ['_'.join(col).strip() for col in agg_df.columns.values]\n","    agg_df.reset_index(drop=True, inplace=True)\n","\n","    times = df.groupby(\"time_window\")[\"timestamp\"].agg([\"min\", \"max\"]).reset_index(drop=True)\n","    agg_df[\"window_start_ms\"] = times[\"min\"]\n","    agg_df[\"window_end_ms\"] = times[\"max\"]\n","\n","    return agg_df\n","\n","\n","def extract_to_csv(input_file, output_csv, label_value, window_seconds=1.0):\n","    \"\"\"Extract + aggregate + append data from 1 JSON file.\"\"\"\n","\n","    # Skip files that are not valid JSON\n","    try:\n","        with open(input_file, \"r\") as f:\n","            data = json.load(f)\n","    except:\n","        print(f\"Skipping (not a JSON file or corrupted): {input_file}\")\n","        return\n","\n","    frames = data.get(\"data\", [])\n","    rows = [extract_frame_features(f) for f in frames if extract_frame_features(f)]\n","\n","    if not rows:\n","        print(f\" No valid frames in {input_file}\")\n","        return\n","\n","    df = pd.DataFrame(rows)\n","    df = compute_velocity_acceleration(df)\n","    agg_df = aggregate_features(df, window_seconds)\n","\n","    # Add label from folder\n","    agg_df[\"result\"] = label_value\n","\n","    file_has_data = os.path.exists(output_csv) and os.path.getsize(output_csv) > 0\n","\n","    agg_df.to_csv(output_csv, mode=\"a\", header=not file_has_data, index=False)\n","\n","    print(f\" Processed {input_file} → {len(agg_df)} aggregated rows\")\n","\n","def process_files_in_subfolders(root_folder, csv_filepath):\n","\n","    # Remove empty CSVs to force header on first write\n","    if os.path.exists(csv_filepath) and os.path.getsize(csv_filepath) == 0:\n","        os.remove(csv_filepath)\n","        print(\"Removed empty CSV so headers can be written fresh.\")\n","\n","    print(\"\\n Scanning folders...\\n\")\n","\n","    for dirpath, dirnames, filenames in os.walk(root_folder):\n","\n","        # Skip the root — we want subfolders as labels\n","        if dirpath == root_folder:\n","            continue\n","\n","        folder_name = os.path.basename(dirpath)\n","        print(f\"\\n Folder detected: {folder_name}\")\n","\n","        for filename in filenames:\n","            if filename.lower().endswith(\".json\"):\n","                json_path = os.path.join(dirpath, filename)\n","                extract_to_csv(json_path, csv_filepath, label_value=folder_name, window_seconds=0.5)\n","\n","if __name__ == \"__main__\":\n","\n","    input_path = \"/content/drive/MyDrive/data before classification\"\n","    output_path = \"/content/drive/MyDrive/mmwave_aggregated.csv\"\n","\n","    process_files_in_subfolders(input_path, output_path)\n","\n","    print(\"\\n DONE! CSV CREATED WITH FULL HEADERS.\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EHxXa-tUztrr","executionInfo":{"status":"ok","timestamp":1763148208116,"user_tz":-330,"elapsed":6660,"user":{"displayName":"Ajit A","userId":"03916160730424160797"}},"outputId":"3f87d463-bb3d-4bd5-f9b6-53e5008214ae"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Scanning folders...\n","\n","\n"," Folder detected: walk and stop\n"," Processed /content/drive/MyDrive/data before classification/walk and stop/replay_9.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walk and stop/replay_3.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walk and stop/replay_1.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walk and stop/replay_4.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walk and stop/replay_11.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walk and stop/replay_2.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walk and stop/replay_10.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walk and stop/replay_7.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walk and stop/replay_6.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walk and stop/replay_8.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walk and stop/replay_5.json → 11 aggregated rows\n","\n"," Folder detected: running\n"," Processed /content/drive/MyDrive/data before classification/running/replay_93.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/running/replay_96.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/running/replay_90.json → 10 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/running/replay_91.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/running/replay_92.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/running/replay_94.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/running/replay_95.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/running/replay_98.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/running/replay_97.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/running/replay_99.json → 11 aggregated rows\n","\n"," Folder detected: walking\n"," Processed /content/drive/MyDrive/data before classification/walking/replay_54.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walking/replay_49.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walking/replay_51.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walking/replay_59.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walking/replay_50.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walking/replay_55.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walking/replay_57.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walking/replay_58.json → 10 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walking/replay_53.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walking/replay_52.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/walking/replay_56.json → 11 aggregated rows\n","\n"," Folder detected: crouching\n"," Processed /content/drive/MyDrive/data before classification/crouching/replay_14.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/crouching/replay_18.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/crouching/replay_16.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/crouching/replay_19.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/crouching/replay_20.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/crouching/replay_15.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/crouching/replay_17.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/crouching/replay_24.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/crouching/replay_23.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/crouching/replay_25.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/crouching/replay_21.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/crouching/replay_22.json → 11 aggregated rows\n"," Processed /content/drive/MyDrive/data before classification/crouching/replay_26.json → 11 aggregated rows\n","\n"," DONE! CSV CREATED WITH FULL HEADERS.\n","\n"]}]}]}